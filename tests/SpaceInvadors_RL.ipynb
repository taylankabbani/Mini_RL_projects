{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an agent that will randomly play the Space Invaders fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: unrar: command not found\r\n"
     ]
    }
   ],
   "source": [
    "# Downloading Atari Env\n",
    "# import urllib.request\n",
    "# urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\n",
    "# !pip install unrar\n",
    "!unrar x Roms.rar\n",
    "# !mkdir rars\n",
    "# !mv HC\\ ROMS.zip   rars\n",
    "# !mv ROMS.zip  rars\n",
    "# !python -m atari_py.import_roms rars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent-environment loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the initial env state\n",
    "cv.imshow('Initial state', env.reset())\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "print(f\"Action space in the environment: {env.action_space}\")\n",
    "print(f\"The state representation that the model will understand: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Score: 115.0\n",
      "Episode: 1, Score: 235.0\n",
      "Episode: 2, Score: 405.0\n",
      "Episode: 3, Score: 180.0\n",
      "Episode: 4, Score: 30.0\n",
      "Episode: 5, Score: 145.0\n",
      "Episode: 6, Score: 225.0\n",
      "Episode: 7, Score: 110.0\n",
      "Episode: 8, Score: 30.0\n",
      "Episode: 9, Score: 105.0\n",
      "Episode: 10, Score: 75.0\n",
      "Episode: 11, Score: 365.0\n",
      "Episode: 12, Score: 105.0\n",
      "Episode: 13, Score: 390.0\n",
      "Episode: 14, Score: 15.0\n",
      "Episode: 15, Score: 210.0\n",
      "Episode: 16, Score: 515.0\n",
      "Episode: 17, Score: 210.0\n",
      "Episode: 18, Score: 670.0\n",
      "Episode: 19, Score: 50.0\n",
      "Episode: 20, Score: 75.0\n",
      "Episode: 21, Score: 155.0\n",
      "Episode: 22, Score: 135.0\n",
      "Episode: 23, Score: 155.0\n",
      "Episode: 24, Score: 180.0\n",
      "Episode: 25, Score: 155.0\n",
      "Episode: 26, Score: 180.0\n",
      "Episode: 27, Score: 95.0\n",
      "Episode: 28, Score: 430.0\n",
      "Episode: 29, Score: 135.0\n",
      "Episode: 30, Score: 75.0\n",
      "Episode: 31, Score: 105.0\n",
      "Episode: 32, Score: 80.0\n",
      "Episode: 33, Score: 290.0\n",
      "Episode: 34, Score: 165.0\n",
      "Episode: 35, Score: 75.0\n",
      "Episode: 36, Score: 110.0\n",
      "Episode: 37, Score: 125.0\n",
      "Episode: 38, Score: 165.0\n",
      "Episode: 39, Score: 210.0\n",
      "Episode: 40, Score: 260.0\n",
      "Episode: 41, Score: 85.0\n",
      "Episode: 42, Score: 275.0\n",
      "Episode: 43, Score: 55.0\n",
      "Episode: 44, Score: 40.0\n",
      "Episode: 45, Score: 60.0\n",
      "Episode: 46, Score: 210.0\n",
      "Episode: 47, Score: 180.0\n",
      "Episode: 48, Score: 130.0\n",
      "Episode: 49, Score: 155.0\n",
      "Episode: 50, Score: 150.0\n",
      "Episode: 51, Score: 125.0\n",
      "Episode: 52, Score: 35.0\n",
      "Episode: 53, Score: 30.0\n",
      "Episode: 54, Score: 145.0\n",
      "Episode: 55, Score: 110.0\n",
      "Episode: 56, Score: 100.0\n",
      "Episode: 57, Score: 145.0\n",
      "Episode: 58, Score: 140.0\n",
      "Episode: 59, Score: 65.0\n",
      "Episode: 60, Score: 145.0\n",
      "Episode: 61, Score: 395.0\n",
      "Episode: 62, Score: 55.0\n",
      "Episode: 63, Score: 115.0\n",
      "Episode: 64, Score: 80.0\n",
      "Episode: 65, Score: 125.0\n",
      "Episode: 66, Score: 195.0\n",
      "Episode: 67, Score: 20.0\n",
      "Episode: 68, Score: 20.0\n",
      "Episode: 69, Score: 120.0\n",
      "Episode: 70, Score: 80.0\n",
      "Episode: 71, Score: 150.0\n",
      "Episode: 72, Score: 330.0\n",
      "Episode: 73, Score: 420.0\n",
      "Episode: 74, Score: 290.0\n",
      "Episode: 75, Score: 50.0\n",
      "Episode: 76, Score: 60.0\n",
      "Episode: 77, Score: 125.0\n",
      "Episode: 78, Score: 75.0\n",
      "Episode: 79, Score: 125.0\n",
      "Episode: 80, Score: 400.0\n",
      "Episode: 81, Score: 210.0\n",
      "Episode: 82, Score: 45.0\n",
      "Episode: 83, Score: 20.0\n",
      "Episode: 84, Score: 70.0\n",
      "Episode: 85, Score: 85.0\n",
      "Episode: 86, Score: 270.0\n",
      "Episode: 87, Score: 210.0\n",
      "Episode: 88, Score: 60.0\n",
      "Episode: 89, Score: 85.0\n",
      "Episode: 90, Score: 160.0\n",
      "Episode: 91, Score: 50.0\n",
      "Episode: 92, Score: 85.0\n",
      "Episode: 93, Score: 215.0\n",
      "Episode: 94, Score: 285.0\n",
      "Episode: 95, Score: 45.0\n",
      "Episode: 96, Score: 80.0\n",
      "Episode: 97, Score: 60.0\n",
      "Episode: 98, Score: 75.0\n",
      "Episode: 99, Score: 105.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "for episode in range (num_episodes):\n",
    "    state_0 = env.reset()\n",
    "    done = False # When true => the agent lost (the end of an episode)\n",
    "    score = 0\n",
    "    num_states_in_episode = 0 \n",
    "    while not done:\n",
    "        env.render() # To show how the agent interact with the env\n",
    "        action = env.action_space.sample() # take a random action\n",
    "        #returns the observations ensued from the agent-env interaction\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building agent that utilize NN to take better actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-15 16:32:47.333599: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-15 16:32:47.333677: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "env=gym.make('SpaceInvaders-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the NN network\n",
    "\n",
    "def build_NN_model(actions,batch_size, hight, width, channels):\n",
    "    nn_input = (batch_size, hight, width, channels)\n",
    "    NN_model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(8,8), strides = (4, 4),activation= \"relu\", input_shape= nn_input),\n",
    "        Conv2D(filters=64, kernel_size= (4,4), strides = (2,2),  activation= \"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(units=512, activation=\"relu\"),\n",
    "        Dense(units=256, activation=\"relu\"),\n",
    "        Dense(units=actions, activation=\"linear\")\n",
    "\n",
    "    ])\n",
    "    return NN_model\n",
    "hight, width, channels = env.observation_space.shape\n",
    "actions= env.action_space.n\n",
    "batch_size = 2\n",
    "NN_model = build_NN_model(actions,batch_size, hight, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the deepQnetwork (agent)\n",
    "from rl.agents import DQNAgent # The DQN algorithm (agent)\n",
    "from rl.memory import SequentialMemory # The Tabular-like structure the agent will use to learn the Q-values\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy  \n",
    "# The policies the RL agent will followes to learn Q-value,  as it's off-policy, the agent will use one greedy\n",
    "# policy to always choose the greedy action (Q-value) and another pilicy that will break the greedy action\n",
    "# selection by rate of $\\epsilon$\n",
    "\n",
    "def build_agent(model, actions, batch_size):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1 ,value_test=.2,\n",
    "                                 nb_steps=100)\n",
    "    memory_s = SequentialMemory(limit=100, window_length=batch_size)\n",
    "    dqn = DQNAgent(model=model, memory=memory_s, policy=policy, nb_actions=actions, enable_dueling_network=True,\n",
    "                   dueling_type='avg', nb_steps_warmup=1000)\n",
    "    return dqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you might get an error related to memory so you need to delete the NN_model from memory (using \n",
    "# del NN_model)and recreate it\n",
    "del NN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = build_agent(NN_model, actions, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN.compile(Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 3000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 1286/10000 [==>...........................] - ETA: 34:57 - reward: 0.2022done, took 310.004 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f85b057c1c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model on the env\n",
    "DQN.fit(env, nb_steps=3000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 25.000, steps: 361\n",
      "Episode 2: reward: 135.000, steps: 656\n",
      "Episode 3: reward: 15.000, steps: 403\n",
      "Episode 4: reward: 315.000, steps: 911\n",
      "Episode 5: reward: 75.000, steps: 956\n",
      "Episode 6: reward: 135.000, steps: 718\n",
      "Episode 7: reward: 25.000, steps: 387\n",
      "Episode 8: reward: 260.000, steps: 1251\n",
      "Episode 9: reward: 150.000, steps: 670\n",
      "Episode 10: reward: 225.000, steps: 906\n"
     ]
    }
   ],
   "source": [
    "## Testing the agent in the env for n episode episodes\n",
    "scores = DQN.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avergae reward for 10 episode is: 136.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The avergae reward for {len(scores.history['nb_steps'])} episode is:\\\n",
    " {np.mean(scores.history['episode_reward'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN.save_weights('models/dqn.hf5')\n",
    "# DQN.load_weights('models/dqn.hf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
