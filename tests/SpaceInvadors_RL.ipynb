{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building an agent that will randomly play the Space Invaders fame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym\r\n",
    "import cv2 as cv"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Downloading Atari Env\r\n",
    "# import urllib.request\r\n",
    "# urllib.request.urlretrieve('http://www.atarimania.com/roms/Roms.rar','Roms.rar')\r\n",
    "# !pip install unrar\r\n",
    "!unrar x Roms.rar\r\n",
    "# !mkdir rars\r\n",
    "# !mv HC\\ ROMS.zip   rars\r\n",
    "# !mv ROMS.zip  rars\r\n",
    "# !python -m atari_py.import_roms rars\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The agent-environment loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env=gym.make('SpaceInvaders-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show the initial env state\r\n",
    "cv.imshow('Initial state', env.reset())\r\n",
    "cv.waitKey(0)\r\n",
    "cv.destroyAllWindows()\r\n",
    "\r\n",
    "print(f\"Action space in the environment: {env.action_space}\")\r\n",
    "print(f\"The state representation that the model will understand: {env.observation_space.shape}\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_episodes = 100\r\n",
    "\r\n",
    "for episode in range (num_episodes):\r\n",
    "    state_0 = env.reset()\r\n",
    "    done = False # When true => the agent lost (the end of an episode)\r\n",
    "    score = 0\r\n",
    "    num_states_in_episode = 0 \r\n",
    "    while not done:\r\n",
    "        env.render() # To show how the agent interact with the env\r\n",
    "        action = env.action_space.sample() # take a random action\r\n",
    "        #returns the observations ensued from the agent-env interaction\r\n",
    "        state, reward, done, info = env.step(action)\r\n",
    "        score += reward\r\n",
    "    print(f\"Episode: {episode}, Score: {score}\")\r\n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building agent that utilize NN to take better actions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "env=gym.make('SpaceInvaders-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Building the NN network\n",
    "\n",
    "def build_NN_model(actions,batch_size, hight, width, channels):\n",
    "    nn_input = (batch_size, hight, width, channels)\n",
    "    NN_model = Sequential([\n",
    "        Conv2D(filters=32, kernel_size=(8,8), strides = (4, 4),activation= \"relu\", input_shape= nn_input),\n",
    "        Conv2D(filters=64, kernel_size= (4,4), strides = (2,2),  activation= \"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(units=512, activation=\"relu\"),\n",
    "        Dense(units=256, activation=\"relu\"),\n",
    "        Dense(units=actions, activation=\"linear\")\n",
    "\n",
    "    ])\n",
    "    return NN_model\n",
    "hight, width, channels = env.observation_space.shape\n",
    "actions= env.action_space.n\n",
    "batch_size = 2\n",
    "NN_model = build_NN_model(actions,batch_size, hight, width, channels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# building the deepQnetwork (agent)\n",
    "from rl.agents import DQNAgent # The DQN algorithm (agent)\n",
    "from rl.memory import SequentialMemory # The Tabular-like structure the agent will use to learn the Q-values\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy  \n",
    "# The policies the RL agent will followes to learn Q-value,  as it's off-policy, the agent will use one greedy\n",
    "# policy to always choose the greedy action (Q-value) and another pilicy that will break the greedy action\n",
    "# selection by rate of $\\epsilon$\n",
    "\n",
    "def build_agent(model, actions, batch_size):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1 ,value_test=.2,\n",
    "                                 nb_steps=100)\n",
    "    memory_s = SequentialMemory(limit=100, window_length=batch_size)\n",
    "    dqn = DQNAgent(model=model, memory=memory_s, policy=policy, nb_actions=actions, enable_dueling_network=True,\n",
    "                   dueling_type='avg', nb_steps_warmup=1000)\n",
    "    return dqn\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Note that you might get an error related to memory so you need to delete the NN_model from memory (using \n",
    "# del NN_model)and recreate it\n",
    "del NN_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DQN = build_agent(NN_model, actions, batch_size=batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DQN.compile(Adam(lr=0.0001))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training the model on the env\n",
    "DQN.fit(env, nb_steps=3000, visualize=False, verbose=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the trained agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Testing the agent in the env for n episode episodes\n",
    "scores = DQN.test(env, nb_episodes=10, visualize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"The avergae reward for {len(scores.history['nb_steps'])} episode is:\\\n",
    " {np.mean(scores.history['episode_reward'])}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving and loading the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DQN.save_weights('models/dqn.hf5')\n",
    "# DQN.load_weights('models/dqn.hf5')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}